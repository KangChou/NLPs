
命名实体识别:

命名实体识别从早期基于词典和规则的方法，到传统机器学习的方法， 后来采用基于深度学习的方法，一直到当下热门的注意力机制、图神经网络等研究方法， 命名实体识别技术路线随着时间在不断发展。

![image](https://user-images.githubusercontent.com/36963108/178393815-01045ee4-885e-4aec-9231-c65cfa3af835.png)


https://github.com/TianRanPig/chinese_ner

https://github.com/CLUEbenchmark/CLUENER2020

https://github.com/hemingkx/CLUENER2020

https://github.com/lonePatient/BERT-NER-Pytorch

https://github.com/lemonhu/NER-BERT-pytorch

https://github.com/google-research/bert

https://github.com/TobiasLee/ChineseNER

https://github.com/PottermoreIron/BERT-BiLSTM-CRF-For-Practice

https://github.com/luopeixiang/named_entity_recognition

https://github.com/F-debug/Medical-named-entity-recognition

https://github.com/kyzhouhzau/BERT-NER

https://github.com/macanv/BERT-BiLSTM-CRF-NER

https://github.com/xuanzebi/BERT-CH-NER

https://github.com/huggingface/transformers

使用bert做领域分类、意图识别和槽位填充任务 https://github.com/xiaopp123/bert-joint-NLU

基于pytorch的中文意图识别和槽位填充 https://github.com/taishan1994/pytorch_bert_intent_classification_and_slot_filling

基于BERT+Tensorflow-1.15+Horovod-0.22的NLU（意图识别+槽位填充）分布式GPU训练模块 https://github.com/jx1100370217/JointBERT_nlu_tf

使用bert做领域分类、配置识别和位置填充任务 https://github.com/xiaopp123/bert-joint-NLU

中文语言理解基准、基准中文语言理解评估基准：数据集、预训练模型、语料库 https://github.com/CLUEbenchmark/CLUE

用于联合意图分类和插槽填充的 BERT https://github.com/monologg/JointBERT

https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification

https://github.com/pymacbit/BERT-Intent-Classification

https://github.com/ensembles4612/medical_intent_detector_using_BERT

https://github.com/AdamLouly/Intent-Classifier-using-BERT-and-TF2/blob/master/BERT2INTENT.ipynb

https://github.com/sz128/slot_filling_and_intent_detection_of_SLU

https://github.com/471417367/bert_intention_zh

数据集自动标注工具--释放AI潜力！https://www.modelfun.cn/home

实体识别数据集 https://github.com/juand-r/entity-recognition-datasets

ner综述： https://blog.csdn.net/weixin_45884316/article/details/118684681

使用 CLIP 将图像和句子嵌入到固定长度的向量中 https://github.com/jina-ai/clip-as-service

other: 

https://github.com/Rhine97/NLP-NER-models/tree/master/JupyterNotebook_Version/dataset

https://github.com/Hyfred/Pytroch_NER_tutorial

https://github.com/cs230-stanford/cs230-code-examples/tree/master/pytorch/nlp

https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py

https://github.com/kamalkraj/BERT-NER


参考：https://github.com/kyzhouhzau/NLPGNN
 ```
[1] BERT：用于语言理解的深度双向转换器的预训练
[2] ALBERT：用于语言表示的自监督学习的 Lite BERT
[3]语言模型是无监督的多任务学习者
[4]用于量子化学的神经消息传递
[ 5]使用图卷积网络进行半监督分类
[6]图注意网络
[7]图神经网络有多强大？
[8] GraphSAGE：大图上的归纳表示学习
[9]扩散改进了图学习
[10]基准图神经网络
[11]用于文本分类的文本级图神经网络
[12]用于文本分类的图卷积网络
[13]用于文本分类的张量图卷积网络
[14]深入了解用于半监督学习的图卷积网络
```

命名实体识别（NER）标注神器: https://blog.csdn.net/qq_44193969/article/details/123298406

实践：https://blog.csdn.net/qq_44193969/article/details/116008734

https://github.com/seanzhang-zhichen/PytorchBilstmCRF-Information-Extraction

https://blog.csdn.net/weixin_40846933/article/details/106384566
